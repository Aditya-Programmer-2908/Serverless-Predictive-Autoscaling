{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":13054542,"sourceType":"datasetVersion","datasetId":8266682}],"dockerImageVersionId":31089,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\n\ndf = pd.read_csv(\"azurefunctions-accesses-2020.csv.bz2\", compression='bz2')\nprint(df.head())","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-09-14T14:42:59.768239Z","iopub.execute_input":"2025-09-14T14:42:59.768628Z","iopub.status.idle":"2025-09-14T14:43:01.735105Z","shell.execute_reply.started":"2025-09-14T14:42:59.768595Z","shell.execute_reply":"2025-09-14T14:43:01.733331Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ----------------------------\n# 1. Suppress TF logs & initialize GPU\n# ----------------------------\nimport os\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'  # suppress INFO/WARNING/ERROR\n\nimport tensorflow as tf\nphysical_devices = tf.config.list_physical_devices('GPU')\nfor gpu in physical_devices:\n    tf.config.experimental.set_memory_growth(gpu, True)\n\n# ----------------------------\n# 2. Import Libraries\n# ----------------------------\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import MinMaxScaler\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import LSTM, Dense\n\n# ----------------------------\n# 3. Load Dataset\n# ----------------------------\ndf = pd.read_csv(\"/kaggle/input/azure-functions/azurefunctions-accesses-2020.csv\")\nprint(\"Columns:\", df.columns)\nprint(df.head())\n\n# ----------------------------\n# 4. Preprocessing\n# ----------------------------\n# Convert timestamp to datetime (milliseconds)\ndf['Timestamp'] = pd.to_datetime(df['Timestamp'], unit='ms')\ndf = df.set_index('Timestamp')\n\n# Resample per minute: count number of invocations\nworkload = df.resample('1min').size().to_frame(name='invocations')\n\n# Fill missing minutes with 0\nworkload = workload.fillna(0)\n\n# Plot workload trend\nplt.figure(figsize=(12,4))\nworkload['invocations'].plot(title=\"Azure Functions Invocations per Minute\")\nplt.show()\n\n# ----------------------------\n# 5. Normalization\n# ----------------------------\nscaler = MinMaxScaler()\nvalues = scaler.fit_transform(workload['invocations'].values.reshape(-1,1))\n\n# ----------------------------\n# 6. Train/Test Split (80/20)\n# ----------------------------\ntrain_size = int(len(values) * 0.8)\ntrain, test = values[:train_size], values[train_size:]\n\n# ----------------------------\n# 7. Sequence Creation\n# ----------------------------\ndef create_sequences(data, seq_length=60):\n    X, y = [], []\n    for i in range(len(data) - seq_length):\n        X.append(data[i:i+seq_length])\n        y.append(data[i+seq_length])\n    return np.array(X), np.array(y)\n\nSEQ_LEN = 60  # past 60 minutes -> predict next minute\nX_train, y_train = create_sequences(train, SEQ_LEN)\nX_test, y_test = create_sequences(test, SEQ_LEN)\n\n# ----------------------------\n# 8. Reshape for LSTM\n# ----------------------------\n# LSTM expects input: (samples, timesteps, features)\nX_train = X_train.reshape((X_train.shape[0], X_train.shape[1], 1))\nX_test = X_test.reshape((X_test.shape[0], X_test.shape[1], 1))\n\nprint(\"Train shape:\", X_train.shape, y_train.shape)\nprint(\"Test shape:\", X_test.shape, y_test.shape)\n\n# ----------------------------\n# 9. Build LSTM Base Model\n# ----------------------------\nmodel = Sequential([\n    LSTM(64, input_shape=(SEQ_LEN,1), return_sequences=False),\n    Dense(32, activation='relu'),\n    Dense(1)  # output: predicted invocations\n])\n\nmodel.compile(optimizer='adam', loss='mse')\nmodel.summary()\n\n# ----------------------------\n# 10. Train Model\n# ----------------------------\nhistory = model.fit(\n    X_train, y_train,\n    epochs=10,\n    batch_size=32,\n    validation_data=(X_test, y_test),\n    verbose=1\n)\n\n# ----------------------------\n# 11. Evaluate Model\n# ----------------------------\ny_pred = model.predict(X_test)\n\nplt.figure(figsize=(12,6))\nplt.plot(y_test[:200], label=\"Actual\")\nplt.plot(y_pred[:200], label=\"Predicted\")\nplt.title(\"Azure Functions - LSTM Base Model Prediction\")\nplt.legend()\nplt.show()\n\n# ----------------------------\n# 12. Save Base Model\n# ----------------------------\nmodel.save(\"lstm_base_model.h5\")\nprint(\"Base model saved as lstm_base_model.h5\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-14T16:11:38.442600Z","iopub.execute_input":"2025-09-14T16:11:38.442907Z","iopub.status.idle":"2025-09-14T16:18:43.573862Z","shell.execute_reply.started":"2025-09-14T16:11:38.442883Z","shell.execute_reply":"2025-09-14T16:18:43.572923Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport torch\nimport torch.nn as nn\nfrom sklearn.preprocessing import MinMaxScaler\nfrom torch.utils.data import Dataset, DataLoader\nimport matplotlib.pyplot as plt\n\n# Load Azure Functions dataset\nDATA_PATH = '/kaggle/input/azure-functions/azurefunctions-accesses-2020.csv'  # Update if your path differs\ndf = pd.read_csv(DATA_PATH)\n\n# Verify dataset columns\nprint(\"Dataset columns:\", df.columns.tolist())\nprint(\"Sample data:\\n\", df.head())\n\n# Preprocessing function (Updated for actual columns)\ndef preprocess_data(df, function_id, sequence_length=10, target_column='count'):\n    \"\"\"\n    Preprocess Azure Functions data for a specific function ID.\n    Derives 'count' as number of invocations per time window.\n    Args:\n        df: DataFrame with Azure Functions data\n        function_id: ID from 'AnonAppName' (e.g., '9gti3olh')\n        sequence_length: Number of time steps in each LSTM sequence\n        target_column: Derived column to predict (e.g., 'count' for invocations)\n    Returns:\n        sequences: Input sequences for LSTM\n        targets: Target values (next invocation count)\n        scaler: MinMaxScaler for inverse transformation\n    \"\"\"\n    # Filter for specific function\n    df_func = df[df['AnonAppName'] == function_id].sort_values(by='Timestamp')\n    \n    # Convert Timestamp to datetime (milliseconds)\n    df_func['Timestamp'] = pd.to_datetime(df_func['Timestamp'], unit='ms')\n    \n    # Aggregate to 1-minute windows: count invocations, fill missing with 0\n    df_func = df_func.set_index('Timestamp').resample('1min').agg({\n        'AnonFunctionInvocationId': 'count'  # Invocation count per minute\n    }).rename(columns={'AnonFunctionInvocationId': 'count'}).fillna(0).reset_index()\n    \n    # Normalize data\n    scaler = MinMaxScaler()\n    data_scaled = scaler.fit_transform(df_func[[target_column]])\n    \n    # Create sequences\n    sequences = []\n    targets = []\n    for i in range(len(data_scaled) - sequence_length):\n        sequences.append(data_scaled[i:i + sequence_length])\n        targets.append(data_scaled[i + sequence_length, 0])\n    \n    return np.array(sequences), np.array(targets), scaler\n\n# Custom Dataset for LSTM\nclass WorkloadDataset(Dataset):\n    def __init__(self, sequences, targets):\n        self.sequences = torch.tensor(sequences, dtype=torch.float32)\n        self.targets = torch.tensor(targets, dtype=torch.float32)\n    \n    def __len__(self):\n        return len(self.sequences)\n    \n    def __getitem__(self, idx):\n        return self.sequences[idx], self.targets[idx]\n\n# LSTM Model\nclass LSTMModel(nn.Module):\n    def __init__(self, input_size=1, hidden_size=64, num_layers=2, output_size=1):\n        super(LSTMModel, self).__init__()\n        self.hidden_size = hidden_size\n        self.num_layers = num_layers\n        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n        self.fc = nn.Linear(hidden_size, output_size)\n    \n    def forward(self, x):\n        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n        out, _ = self.lstm(x, (h0, c0))\n        out = self.fc(out[:, -1, :])\n        return out\n\n# Training function\ndef train_lstm(model, train_loader, criterion, optimizer, num_epochs=10, device='cuda' if torch.cuda.is_available() else 'cpu'):\n    model = model.to(device)\n    model.train()\n    losses = []\n    \n    for epoch in range(num_epochs):\n        epoch_loss = 0\n        for sequences, targets in train_loader:\n            sequences, targets = sequences.to(device), targets.to(device)\n            optimizer.zero_grad()\n            outputs = model(sequences)\n            loss = criterion(outputs.squeeze(), targets)\n            loss.backward()\n            optimizer.step()\n            epoch_loss += loss.item()\n        avg_loss = epoch_loss / len(train_loader)\n        losses.append(avg_loss)\n        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {avg_loss:.4f}')\n    \n    return losses\n\n# Main execution\nif __name__ == \"__main__\":\n    # Select a function ID from 'AnonAppName' (use first unique for testing)\n    unique_functions = df['AnonAppName'].unique()\n    function_id = unique_functions[0]  # e.g., '9gti3olh'\n    print(f\"Available functions: {unique_functions[:5]}...\")  # Show first 5\n    print(f\"Training LSTM for function: {function_id}\")\n    \n    # Preprocess data\n    sequences, targets, scaler = preprocess_data(df, function_id=function_id, sequence_length=10, target_column='count')\n    print(f\"Processed data shape: Sequences {sequences.shape}, Targets {targets.shape}\")\n    \n    # Create dataset and dataloader\n    dataset = WorkloadDataset(sequences, targets)\n    train_loader = DataLoader(dataset, batch_size=32, shuffle=True)\n    \n    # Initialize model, criterion, optimizer\n    model = LSTMModel(input_size=1, hidden_size=64, num_layers=2, output_size=1)\n    criterion = nn.MSELoss()\n    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n    \n    # Train model\n    losses = train_lstm(model, train_loader, criterion, optimizer, num_epochs=10)\n    \n    # Save model\n    torch.save(model.state_dict(), '/kaggle/working/lstm_model.pth')\n    print(\"Model saved to /kaggle/working/lstm_model.pth\")\n    \n    # Plot training loss\n    plt.figure(figsize=(8, 4))\n    plt.plot(losses)\n    plt.title('LSTM Training Loss')\n    plt.xlabel('Epoch')\n    plt.ylabel('Loss')\n    plt.show()\n    \n    # Test prediction\n    model.eval()\n    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n    test_sequence = torch.tensor(sequences[-1:], dtype=torch.float32).to(device)\n    with torch.no_grad():\n        pred_scaled = model(test_sequence).cpu().numpy()\n    pred_original = scaler.inverse_transform(pred_scaled.reshape(-1, 1))[0, 0]\n    print(f'Predicted next invocations (original scale): {pred_original:.2f}')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-14T16:41:42.601504Z","iopub.execute_input":"2025-09-14T16:41:42.602020Z","iopub.status.idle":"2025-09-14T16:44:39.735227Z","shell.execute_reply.started":"2025-09-14T16:41:42.601978Z","shell.execute_reply":"2025-09-14T16:44:39.734300Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}